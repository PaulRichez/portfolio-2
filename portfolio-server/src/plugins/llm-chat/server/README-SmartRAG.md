# SmartRAGTool - RAG Intelligent avec Ollama

## ğŸ¯ Objectif

Le **SmartRAGTool** rÃ©volutionne l'utilisation du RAG (Retrieval-Augmented Generation) en utilisant l'intelligence artificielle pour dÃ©cider automatiquement quand effectuer une recherche dans la base de donnÃ©es ChromaDB.

## ğŸ§  Intelligence Artificielle

### ModÃ¨le UtilisÃ©
- **Nom** : `qwen3:0.6b` (Qwen 3 - 0.6 milliards de paramÃ¨tres)
- **Taille** : ~522MB
- **Vitesse** : TrÃ¨s rapide (~100ms par analyse)
- **PrÃ©cision** : Excellente pour l'analyse de pertinence

### CapacitÃ©s d'Analyse
âœ… **DÃ©tection automatique** de la pertinence des questions  
âœ… **Extraction intelligente** des mots-clÃ©s  
âœ… **Ã‰valuation de confiance** (0-100%)  
âœ… **Raisonnement expliquÃ©** pour chaque dÃ©cision  
âœ… **Fallback robuste** en cas d'erreur  

## ğŸš€ DÃ©marrage Rapide

### 1. VÃ©rification du SystÃ¨me
```bash
# VÃ©rifier que Ollama fonctionne
node src/plugins/llm-chat/server/scripts/check-ollama.js

# Test complet de l'intÃ©gration
node src/plugins/llm-chat/server/scripts/test-ollama-integration.js
```

### 2. Utilisation dans le Code
```typescript
import { SmartRAGTool } from '../tools/smart-rag-tool';

// CrÃ©er l'outil
const smartRAG = new SmartRAGTool(strapi);

// Analyser une question
const result = await smartRAG._call("Quels sont tes projets React ?");
console.log(result);
```

## ğŸ“Š Exemples d'Analyse IA

### âœ… Questions NÃ©cessitant RAG
```typescript
// Question: "Quels sont tes projets React ?"
{
  shouldUseRAG: true,
  confidence: 0.95,
  keywords: ["projets", "React"],
  reasoning: "Question spÃ©cifique sur les projets avec technologie mentionnÃ©e"
}

// Question: "Comment te contacter ?"
{
  shouldUseRAG: true,
  confidence: 0.90,
  keywords: ["contact"],
  reasoning: "Demande d'informations de contact stockÃ©es en base"
}
```

### âŒ Questions Ne NÃ©cessitant Pas RAG
```typescript
// Question: "Quel temps fait-il ?"
{
  shouldUseRAG: false,
  confidence: 0.98,
  keywords: [],
  reasoning: "Question mÃ©tÃ©orologique non liÃ©e au portfolio"
}

// Question: "Que penses-tu de la politique ?"
{
  shouldUseRAG: false,
  confidence: 0.85,
  keywords: [],
  reasoning: "Question d'opinion gÃ©nÃ©rale non professionnelle"
}
```

## ğŸ”§ Configuration

### ParamÃ¨tres Ollama
```typescript
// Dans ollama-service.ts
const config = {
  baseUrl: 'http://localhost:11434',
  qwenModel: 'qwen3:0.6b',
  timeout: 10000
};

// Options de gÃ©nÃ©ration
const options = {
  temperature: 0.1,     // Faible pour cohÃ©rence
  num_ctx: 2048,        // Contexte suffisant
  top_p: 0.9           // Ã‰chantillonnage focused
};
```

### Prompt d'Analyse
Le prompt est optimisÃ© pour `qwen3:0.6b` et guide prÃ©cisÃ©ment l'IA :

```
Tu es un systÃ¨me d'analyse intelligent qui dÃ©termine si une question 
nÃ©cessite une recherche dans une base de donnÃ©es de portfolio.

QUESTION Ã€ ANALYSER: "[question]"

CONTEXTE: La base de donnÃ©es contient...
- Projets de dÃ©veloppement
- CompÃ©tences techniques  
- ExpÃ©riences professionnelles
- Formation et Ã©ducation
- Informations de contact

RÃ‰PONDS UNIQUEMENT au format JSON suivant :
{
  "shouldUseRAG": true/false,
  "confidence": 0.0-1.0,
  "keywords": ["mot1", "mot2"],
  "reasoning": "explication courte"
}
```

## ğŸ›¡ï¸ SystÃ¨me de Fallback

En cas d'Ã©chec d'Ollama, le systÃ¨me bascule automatiquement :

```typescript
// Analyse manuelle de secours
private shouldUseRAGFallback(message: string): boolean {
  const portfolioKeywords = [
    'projet', 'projects', 'compÃ©tence', 'skills',
    'expÃ©rience', 'formation', 'contact',
    'react', 'vue', 'angular', 'nodejs', 'php'
  ];
  
  return portfolioKeywords.some(keyword => 
    message.toLowerCase().includes(keyword)
  );
}
```

## ğŸ“ˆ Performance et Monitoring

### MÃ©triques ClÃ©s
- **Temps d'analyse** : <200ms (cible)
- **Taux de succÃ¨s Ollama** : >95% (cible)
- **PrÃ©cision des dÃ©cisions** : >90% (Ã©valuation manuelle)

### Logs de Surveillance
```
ğŸ¤– SmartRAGTool: Starting AI analysis with Ollama qwen3:0.6b
ğŸ§  Ollama analysis result: { shouldUseRAG: true, confidence: 0.95 }
ğŸ¯ AI-generated search query: "projets React"
â±ï¸  SmartRAG AI Search: 45ms
âœ… Found 3 relevant documents using AI analysis
```

### SantÃ© du SystÃ¨me
```bash
# Check rapide
curl http://localhost:11434/api/tags

# Test des modÃ¨les
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen3:0.6b", "prompt": "Test", "stream": false}'
```

## ğŸ”„ IntÃ©gration avec LangChain

### Agent OpenAI (avec outils)
```typescript
const tools = [
  new SmartRAGTool(strapi),           // âœ¨ Principal - IA
  new ChromaRetrievalTool(strapi),    // ğŸ”§ Support - Simple  
  new ChromaAdvancedRetrievalTool(strapi) // ğŸ”§ Support - AvancÃ©
];

const agent = await createOpenAIFunctionsAgent({
  llm: model,
  tools,
  prompt: agentPrompt
});
```

### ModÃ¨les Custom (RAG Smart)
```typescript
conversationChains.set(sessionId, {
  type: 'rag_smart',
  model: createModel(config, options),
  smartRAGTool: new SmartRAGTool(strapi),
  memory
});
```

## ğŸš¨ DÃ©pannage

### ProblÃ¨mes Courants

#### 1. Ollama non accessible
```bash
# VÃ©rifier le service
docker ps | grep ollama

# RedÃ©marrer si nÃ©cessaire
docker-compose restart ollama
```

#### 2. ModÃ¨le manquant
```bash
# Installer qwen3:0.6b
docker exec portfolio-ollama ollama pull qwen3:0.6b

# VÃ©rifier l'installation
docker exec portfolio-ollama ollama list
```

#### 3. Analyses incohÃ©rentes
- VÃ©rifier la **tempÃ©rature** (doit Ãªtre basse: 0.1)
- Tester le **prompt** manuellement
- ConsidÃ©rer un **fine-tuning** si nÃ©cessaire

#### 4. Performance lente
- VÃ©rifier les **ressources** Docker (RAM/CPU)
- Optimiser le **contexte** (num_ctx)
- Utiliser le **fallback** temporairement

### Logs d'Erreur Typiques
```
âŒ Ollama analysis failed, using fallback: ECONNREFUSED
ğŸ”„ Fallback analysis used (Ollama error: Connection refused)
âš ï¸  No JSON found in Ollama response
```

## ğŸ“š Ressources

### Documentation
- [Architecture RAG](./docs/RAG-INTEGRATION.md)
- [Configuration Ollama](./docs/OLLAMA-SETUP.md)
- [Prompts Templates](./prompts/)

### Scripts Utiles
- `check-ollama.js` - VÃ©rification rapide
- `test-ollama-integration.js` - Tests complets
- `benchmark-models.js` - Comparaison de modÃ¨les

### Monitoring
- Logs Strapi : `/logs/`
- MÃ©triques Ollama : `http://localhost:11434/`
- Tests automatisÃ©s : CI/CD pipeline

## ğŸ‰ Avantages de cette Approche

### ğŸ¯ **Intelligence**
- DÃ©cisions basÃ©es sur l'IA, pas sur des rÃ¨gles fixes
- Adaptation automatique aux nouveaux types de questions
- AmÃ©lioration continue de la prÃ©cision

### âš¡ **Performance**
- ModÃ¨le lÃ©ger et rapide (qwen3:0.6b)
- Cache des conversations pour Ã©viter les re-analyses
- Fallback instantanÃ© en cas de problÃ¨me

### ğŸ”„ **FlexibilitÃ©**
- Compatible avec tous les providers LLM
- Extensible pour nouveaux domaines
- Configuration adaptable selon les besoins

### ğŸ›¡ï¸ **Robustesse**
- SystÃ¨me de fallback automatique
- Logging complet pour le dÃ©bogage
- Graceful degradation garantie

---

> ğŸ’¡ **Note** : Ce systÃ¨me reprÃ©sente une Ã©volution majeure vers un RAG vÃ©ritablement intelligent, oÃ¹ l'IA dÃ©cide elle-mÃªme quand et comment rechercher l'information pertinente.
