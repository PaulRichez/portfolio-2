import type { Core } from '@strapi/strapi';
import { ChatOpenAI } from "@langchain/openai";
import { PromptTemplate } from "@langchain/core/prompts";
import { LLMChain } from "langchain/chains";
import { ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate } from "@langchain/core/prompts";
import { ConversationChain } from "langchain/chains";
import { BaseChatMemory } from "langchain/memory";
import { BaseMessage, AIMessage, HumanMessage } from "@langchain/core/messages";
// Imports pour les outils LangChain
import { AgentExecutor, createOpenAIFunctionsAgent } from "langchain/agents";
import { ChromaRetrievalTool, ChromaAdvancedRetrievalTool } from "../tools/chroma-retrieval-tool";

// Interface pour d√©finir la structure de la configuration
export interface LlmChatConfig {
  provider: 'openai' | 'custom';
  openai: {
    apiKey: string;
    modelName: string;
    temperature: number;
  };
  custom: {
    baseUrl: string;
    modelName: string;
    temperature: number;
    apiKey: string;
  };
}

// Interface pour les options de conversation
export interface ConversationOptions {
  sessionId?: string;
  systemPrompt?: string;
  maxTokens?: number;
  temperature?: number;
  useRAG?: boolean; // Nouvelle option pour activer/d√©sactiver RAG
}

// M√©moire personnalis√©e utilisant Strapi
class StrapiChatMemory extends BaseChatMemory {
  private strapi: Core.Strapi;
  private sessionId: string;

  constructor(strapi: Core.Strapi, sessionId: string) {
    super({ returnMessages: true, inputKey: "input", outputKey: "response" });
    this.strapi = strapi;
    this.sessionId = sessionId;
  }

  get memoryKeys() {
    return ["history"];
  }

  async loadMemoryVariables() {
    const messages = await this.getChatMessages();
    return { history: messages };
  }

  async saveContext(inputValues: Record<string, any>, outputValues: Record<string, any>) {
    const input = inputValues[this.inputKey];
    const output = outputValues[this.outputKey];

    strapi.log.info('üíæ Saving context for session:', this.sessionId);
    console.log('üíæ Saving context for session:', this.sessionId);
    console.log('User message:', input.substring(0, 50) + '...');
    console.log('Assistant response:', output.substring(0, 50) + '...');

    try {
      // V√©rifier que le content-type existe
      const messageContentType = strapi.contentType('plugin::llm-chat.chat-message');
      if (!messageContentType) {
        throw new Error('Content type plugin::llm-chat.chat-message not found');
      }
      console.log('‚úÖ Content type chat-message found');

      // Sauvegarder le message utilisateur
      const userMessage = await this.strapi.entityService.create('plugin::llm-chat.chat-message', {
        data: {
          sessionId: this.sessionId,
          role: 'user',
          content: input,
          timestamp: new Date().toISOString()
        }
      });
      strapi.log.info('‚úÖ User message saved with ID:', userMessage.id);
      console.log('‚úÖ User message saved with ID:', userMessage.id);

      // Sauvegarder la r√©ponse de l'assistant
      const assistantMessage = await this.strapi.entityService.create('plugin::llm-chat.chat-message', {
        data: {
          sessionId: this.sessionId,
          role: 'assistant',
          content: output,
          timestamp: new Date().toISOString()
        }
      });
      strapi.log.info('‚úÖ Assistant message saved with ID:', assistantMessage.id);
      console.log('‚úÖ Assistant message saved with ID:', assistantMessage.id);

      // Cr√©er ou mettre √† jour la session
      await this.updateOrCreateSession(input, output);
    } catch (error) {
      strapi.log.error('‚ùå Error saving messages:', error);
      console.error('‚ùå Error saving messages:', error);
      throw error;
    }
  }

  async updateOrCreateSession(userMessage: string, assistantResponse: string) {
    try {
      console.log('üîÑ Updating session:', this.sessionId);

      // V√©rifier si la session existe
      const existingSession = await this.strapi.entityService.findMany('plugin::llm-chat.chat-session', {
        filters: { sessionId: this.sessionId }
      }) as any[];

      const messageCount = await this.strapi.entityService.count('plugin::llm-chat.chat-message', {
        filters: { sessionId: this.sessionId }
      });

      console.log('üìä Current message count for session:', messageCount);

      const sessionData = {
        sessionId: this.sessionId,
        messageCount,
        lastActivity: new Date().toISOString(),
        lastMessage: assistantResponse.substring(0, 100)
      } as any;

      if (existingSession.length > 0) {
        // Mettre √† jour la session existante
        console.log('üìù Updating existing session...');
        await this.strapi.entityService.update('plugin::llm-chat.chat-session', existingSession[0].id, {
          data: sessionData
        });
        console.log('‚úÖ Session updated');
      } else {
        // Cr√©er une nouvelle session
        console.log('üÜï Creating new session...');
        const newSession = await this.strapi.entityService.create('plugin::llm-chat.chat-session', {
          data: {
            ...sessionData,
            title: userMessage.substring(0, 50) + (userMessage.length > 50 ? '...' : '')
          }
        });
        console.log('‚úÖ New session created with ID:', newSession.id);
      }
    } catch (error) {
      console.error('‚ùå Error updating session:', error);
    }
  }

  async getChatMessages(): Promise<BaseMessage[]> {
    const messages = await this.strapi.entityService.findMany('plugin::llm-chat.chat-message', {
      filters: { sessionId: this.sessionId },
      sort: { createdAt: 'asc' }
    });

    return messages.map((msg: any) => {
      if (msg.role === 'user') {
        return new HumanMessage(msg.content);
      } else {
        return new AIMessage(msg.content);
      }
    });
  }

  async clear() {
    const messages = await this.strapi.entityService.findMany('plugin::llm-chat.chat-message', {
      filters: { sessionId: this.sessionId }
    }) as any[];

    for (const message of messages) {
      await this.strapi.entityService.delete('plugin::llm-chat.chat-message', message.id);
    }
  }
}

const langchainService = ({ strapi }: { strapi: Core.Strapi }) => {
  // Stocker les cha√Ænes de conversation en cache
  const conversationChains = new Map();

  // Cr√©er un mod√®le en fonction de la configuration
  const createModel = (config: LlmChatConfig, options?: ConversationOptions) => {
    const temperature = options?.temperature !== undefined
      ? Number(options.temperature)
      : Number(config.provider === 'openai' ? config.openai.temperature : config.custom.temperature);

    if (config.provider === 'openai') {
      return new ChatOpenAI({
        modelName: config.openai.modelName,
        temperature: temperature,
        openAIApiKey: config.openai.apiKey,
        maxTokens: options?.maxTokens ? Number(options.maxTokens) : undefined,
      });
    } else if (config.provider === 'custom') {
      return new ChatOpenAI({
        modelName: config.custom.modelName,
        temperature: temperature,
        maxTokens: options?.maxTokens ? Number(options.maxTokens) : undefined,
        configuration: {
          baseURL: config.custom.baseUrl,
          apiKey: config.custom.apiKey || "not-needed",
        },
      });
    } else {
      throw new Error(`Unsupported LLM provider: ${config.provider}`);
    }
  };

  // M√©thodes utilitaires pour le RAG manuel
  const shouldUseRAG = (message: string): boolean => {
    const portfolioKeywords = [
      'projet', 'projects', 'comp√©tence', 'skills', 'exp√©rience', 'experience',
      'formation', 'education', 'contact', 'r√©alisation', 'portfolio',
      'technologie', 'technology', 'd√©veloppement', 'development',
      'react', 'vue', 'angular', 'nodejs', 'php', 'python', 'javascript',
      'typescript', 'html', 'css', 'bootstrap', 'tailwind',
      'qui es-tu', 'pr√©sente', 'cv', 'profil', 'about', '√† propos',
      'github', 'linkedin', 'email', 't√©l√©phone', 'coordonn√©es',
      'web', 'mobile', 'frontend', 'backend', 'fullstack'
    ];

    const lowerMessage = message.toLowerCase();
    return portfolioKeywords.some(keyword => lowerMessage.includes(keyword));
  };

  const formatChromaResults = (results: any[], originalQuery: string): string => {
    if (!results || results.length === 0) {
      return '';
    }

    const sections: string[] = [];

    sections.push(`=== Informations contextuelles pour "${originalQuery}" ===\n`);

    results.forEach((result, index) => {
      const metadata = result.metadata || {};
      const collection = metadata.collection || 'unknown';
      const similarity = (1 - result.distance).toFixed(3); // Convertir distance en similarit√©

      sections.push(`${index + 1}. ${getCollectionDisplayName(collection)} (Pertinence: ${similarity})`);
      sections.push(`   ${result.document.trim()}`);

      // Ajouter des m√©tadonn√©es pertinentes
      if (metadata.github_link) {
        sections.push(`   üîó GitHub: ${metadata.github_link}`);
      }
      if (metadata.link_demo) {
        sections.push(`   üåê D√©mo: ${metadata.link_demo}`);
      }
      if (metadata.email) {
        sections.push(`   üìß Email: ${metadata.email}`);
      }
      if (metadata.linkedin) {
        sections.push(`   üíº LinkedIn: ${metadata.linkedin}`);
      }
      if (metadata.website) {
        sections.push(`   üåê Site web: ${metadata.website}`);
      }
      if (metadata.phoneNumber) {
        sections.push(`   üìû T√©l√©phone: ${metadata.phoneNumber}`);
      }

      sections.push(''); // Ligne vide entre les r√©sultats
    });

    sections.push(`=== Fin des informations contextuelles (${results.length} r√©sultat${results.length > 1 ? 's' : ''}) ===\n`);

    return sections.join('\n');
  };

  const getCollectionDisplayName = (collection: string): string => {
    const displayNames: Record<string, string> = {
      'api::project.project': 'üìÅ Projet',
      'api::me.me': 'üë§ Profil personnel',
      'api::article.article': 'üìù Article',
      'api::faq.faq': '‚ùì FAQ'
    };

    return displayNames[collection] || `üìÑ ${collection}`;
  };

  return {
    // Cr√©er une nouvelle conversation ou continuer une existante
    async chat(message: string, options?: ConversationOptions) {
      try {
        strapi.log.info('üöÄ Starting chat for session:', options?.sessionId);
        console.log('üöÄ Starting chat for session:', options?.sessionId);
        console.log('üìù User message:', message.substring(0, 50) + '...');

        // V√©rifier que Strapi est correctement initialis√©
        if (!strapi.entityService) {
          throw new Error('Strapi entity service not available');
        }

        const pluginConfig = strapi.config.get('plugin::llm-chat') || strapi.plugin('llm-chat').config('default');

        if (!pluginConfig) {
          throw new Error('LLM Chat plugin configuration not found');
        }

        const config = pluginConfig as LlmChatConfig;

        if (!config.provider) {
          throw new Error('LLM provider not configured');
        }

        const sessionId = options?.sessionId || 'default';
        const model = createModel(config, options);

        // S'assurer qu'une session existe AVANT de cr√©er la cha√Æne
        await this.ensureSessionExists(sessionId, message);

        // Test: cr√©er un message directement pour voir si √ßa fonctionne
        try {
          const testMessage = await strapi.entityService.create('plugin::llm-chat.chat-message', {
            data: {
              sessionId: sessionId,
              role: 'system',
              content: 'Test message',
              timestamp: new Date().toISOString()
            }
          });
          console.log('‚úÖ Test message created successfully:', testMessage.id);

          // Supprimer le message de test
          await strapi.entityService.delete('plugin::llm-chat.chat-message', testMessage.id);
          console.log('‚úÖ Test message deleted');
        } catch (testError) {
          console.error('‚ùå Failed to create test message:', testError);
          throw new Error('Database connection or content-type issue: ' + testError.message);
        }        // R√©cup√©rer ou cr√©er une conversation
        if (!conversationChains.has(sessionId)) {
          console.log('üîó Creating new conversation chain for session:', sessionId);

          // Cr√©er une m√©moire personnalis√©e utilisant Strapi
          const memory = new StrapiChatMemory(strapi, sessionId);

          // D√©cider si on utilise un agent avec outils RAG ou une conversation simple
          const useRAG = options?.useRAG !== false; // RAG activ√© par d√©faut

          if (useRAG) {
            if (config.provider === 'openai') {
              console.log('ü§ñ Creating OpenAI agent with ChromaDB tools...');

              // Cr√©er les outils ChromaDB
              const tools = [
                new ChromaRetrievalTool(strapi),
                new ChromaAdvancedRetrievalTool(strapi)
              ];

              // Prompt syst√®me pour l'agent avec RAG
              const systemPrompt = options?.systemPrompt || `Tu es un assistant IA sp√©cialis√© dans le portfolio et les informations personnelles.

INSTRUCTIONS IMPORTANTES :
1. Utilise l'outil 'chroma_search' pour rechercher des informations pertinentes dans la base de donn√©es quand l'utilisateur :
   - Pose des questions sur les projets
   - Demande des informations personnelles, comp√©tences, exp√©riences
   - Cherche des d√©tails sp√©cifiques sur le portfolio

2. R√©ponds toujours en fran√ßais de mani√®re naturelle et conversationnelle
3. Utilise les informations trouv√©es pour donner des r√©ponses compl√®tes et pr√©cises
4. Si tu ne trouves pas d'informations pertinentes, dis-le clairement
5. Inclus les liens et d√©tails pertinents quand ils sont disponibles

Tu peux rechercher des informations sur :
- Les projets de d√©veloppement
- Les comp√©tences techniques
- L'exp√©rience professionnelle
- La formation
- Les coordonn√©es et liens sociaux`;

              const agentPrompt = ChatPromptTemplate.fromMessages([
                ["system", systemPrompt],
                new MessagesPlaceholder("chat_history"),
                ["human", "{input}"],
                new MessagesPlaceholder("agent_scratchpad"),
              ]);

              // Cr√©er l'agent OpenAI Functions
              const agent = await createOpenAIFunctionsAgent({
                llm: model,
                tools,
                prompt: agentPrompt,
              });

              // Cr√©er l'ex√©cuteur d'agent avec m√©moire personnalis√©e
              const agentExecutor = new AgentExecutor({
                agent,
                tools,
                memory,
                verbose: true,
                returnIntermediateSteps: false,
              });

              conversationChains.set(sessionId, { type: 'agent', executor: agentExecutor });
            } else {
              console.log('üîß Creating custom RAG chain with manual tool integration...');

              // Pour les mod√®les custom (Ollama), on utilise une approche RAG manuelle
              const chromaService = strapi.plugin('llm-chat').service('chromaVectorService');

              // Prompt syst√®me pour RAG manuel
              const systemPrompt = options?.systemPrompt || `Tu es un assistant IA sp√©cialis√© dans le portfolio et les informations personnelles.

Tu as acc√®s √† une base de donn√©es de connaissances sur le portfolio. Quand l'utilisateur pose des questions sur :
- Les projets de d√©veloppement
- Les comp√©tences techniques
- L'exp√©rience professionnelle
- La formation
- Les coordonn√©es et informations de contact

Tu recevras automatiquement des informations contextuelles pertinentes de la base de donn√©es.

R√©ponds toujours en fran√ßais de mani√®re naturelle et conversationnelle.
Utilise les informations contextuelles fournies pour donner des r√©ponses compl√®tes et pr√©cises.
Si aucune information contextuelle n'est fournie, r√©ponds avec tes connaissances g√©n√©rales.`;

              const chatPrompt = ChatPromptTemplate.fromMessages([
                ["system", systemPrompt],
                new MessagesPlaceholder("history"),
                HumanMessagePromptTemplate.fromTemplate("{context}\n\nQuestion: {input}"),
              ]);

              const chain = new ConversationChain({
                memory: memory,
                prompt: chatPrompt,
                llm: model,
              });

              conversationChains.set(sessionId, {
                type: 'rag_manual',
                chain,
                chromaService
              });
            }
          } else {
            console.log('üí¨ Creating simple conversation chain...');

            // Conversation simple sans outils
            const systemPrompt = options?.systemPrompt || "Tu es un assistant IA utile qui r√©pond en fran√ßais.";
            const chatPrompt = ChatPromptTemplate.fromMessages([
              ["system", systemPrompt],
              new MessagesPlaceholder("history"),
              HumanMessagePromptTemplate.fromTemplate("{input}"),
            ]);

            const chain = new ConversationChain({
              memory: memory,
              prompt: chatPrompt,
              llm: model,
            });

            conversationChains.set(sessionId, { type: 'chain', chain });
          }
        } else {
          console.log('‚ôªÔ∏è Using existing conversation for session:', sessionId);
        }

        // R√©cup√©rer la conversation existante
        const conversationData = conversationChains.get(sessionId);

        console.log('‚ö° Calling LLM...');

        let response;
        if (conversationData.type === 'agent') {
          // Utiliser l'agent avec outils
          response = await conversationData.executor.call({
            input: message,
          });
        } else if (conversationData.type === 'rag_manual') {
          // Utiliser RAG manuel pour les mod√®les custom
          console.log('üîç Using manual RAG for custom provider...');

          // Fonction pour d√©tecter si on a besoin de rechercher dans ChromaDB
          const needsRAG = shouldUseRAG(message);

          let context = '';
          if (needsRAG) {
            console.log('üïµÔ∏è Searching ChromaDB for relevant information...');
            try {
              const searchResults = await conversationData.chromaService.searchDocuments(message, 5);
              if (searchResults && searchResults.length > 0) {
                context = formatChromaResults(searchResults, message);
                console.log(`‚úÖ Found ${searchResults.length} relevant documents`);
              } else {
                console.log('‚ÑπÔ∏è No relevant documents found in ChromaDB');
              }
            } catch (searchError) {
              console.error('‚ùå Error searching ChromaDB:', searchError);
            }
          } else {
            console.log('‚ÑπÔ∏è Question does not require ChromaDB search');
          }

          // Appeler la cha√Æne avec le contexte
          response = await conversationData.chain.call({
            input: message,
            context: context
          });
        } else {
          // Utiliser la cha√Æne simple
          response = await conversationData.chain.call({
            input: message,
          });
        }

        console.log('‚úÖ LLM response received');

        // V√©rifier que les messages ont bien √©t√© sauvegard√©s
        const messages = await strapi.entityService.findMany('plugin::llm-chat.chat-message', {
          filters: { sessionId },
          sort: { createdAt: 'asc' }
        });

        console.log('üìö Total messages in database for session:', messages.length);

        // V√©rifier que la session existe
        const sessions = await strapi.entityService.findMany('plugin::llm-chat.chat-session', {
          filters: { sessionId }
        });

        console.log('üóÇÔ∏è Sessions found:', sessions.length);

        return {
          sessionId,
          response: response.response,
          history: messages,
        };
      } catch (error) {
        strapi.log.error('‚ùå Error in langchain chat service:', error);
        console.error('‚ùå Error in langchain chat service:', error);
        throw error;
      }
    },

    // S'assurer qu'une session existe
    async ensureSessionExists(sessionId: string, firstMessage: string) {
      try {
        strapi.log.info('üîç Checking if session exists:', sessionId);
        console.log('üîç Checking if session exists:', sessionId);

        // V√©rifier que le content-type existe
        try {
          const contentType = strapi.contentType('plugin::llm-chat.chat-session');
          if (!contentType) {
            throw new Error('Content type plugin::llm-chat.chat-session not found');
          }
          console.log('‚úÖ Content type chat-session found');
        } catch (error) {
          console.error('‚ùå Content type chat-session not found:', error);
          throw error;
        }

        const existingSession = await strapi.entityService.findMany('plugin::llm-chat.chat-session', {
          filters: { sessionId }
        }) as any[];

        console.log('üîç Query result:', existingSession);

        if (existingSession.length === 0) {
          strapi.log.info('üÜï Session does not exist, creating new one...');
          console.log('üÜï Session does not exist, creating new one...');

          // Cr√©er une nouvelle session
          const newSession = await strapi.entityService.create('plugin::llm-chat.chat-session', {
            data: {
              sessionId,
              title: firstMessage.substring(0, 50) + (firstMessage.length > 50 ? '...' : ''),
              messageCount: 0,
              lastActivity: new Date().toISOString(),
              lastMessage: 'New session'
            }
          });

          strapi.log.info('‚úÖ New session created with ID:', newSession.id);
          console.log('‚úÖ New session created with ID:', newSession.id);
        } else {
          strapi.log.info('‚úÖ Session already exists');
          console.log('‚úÖ Session already exists');
        }
      } catch (error) {
        strapi.log.error('‚ùå Error ensuring session exists:', error);
        console.error('‚ùå Error ensuring session exists:', error);
        throw error;
      }
    },

    // Obtenir l'historique d'une conversation
    async getHistory(sessionId: string = 'default') {
      try {
        const messages = await strapi.entityService.findMany('plugin::llm-chat.chat-message', {
          filters: { sessionId },
          sort: { createdAt: 'asc' }
        });

        return {
          sessionId,
          messages,
          messageCount: messages.length,
          lastActivity: messages.length > 0 ? messages[messages.length - 1].timestamp : null
        };
      } catch (error) {
        strapi.log.error('Error getting history:', error);
        return {
          sessionId,
          messages: [],
          messageCount: 0,
          lastActivity: null
        };
      }
    },

    // Obtenir toutes les sessions actives
    async getAllSessions() {
      try {
        // R√©cup√©rer toutes les sessions distinctes
        const sessions = await strapi.db.query('plugin::llm-chat.chat-session').findMany({
          orderBy: { updatedAt: 'desc' }
        });

        return sessions.map(session => ({
          sessionId: session.sessionId,
          title: session.title || null,
          messageCount: session.messageCount || 0,
          lastActivity: session.lastActivity || session.updatedAt,
          lastMessage: session.lastMessage || 'No messages'
        }));
      } catch (error) {
        strapi.log.error('Error getting all sessions:', error);
        return [];
      }
    },

    // Effacer l'historique d'une conversation
    async clearHistory(sessionId: string = 'default') {
      try {
        const messages = await strapi.entityService.findMany('plugin::llm-chat.chat-message', {
          filters: { sessionId }
        }) as any[];

        for (const message of messages) {
          await strapi.entityService.delete('plugin::llm-chat.chat-message', message.id);
        }

        // Supprimer aussi la session
        const sessions = await strapi.entityService.findMany('plugin::llm-chat.chat-session', {
          filters: { sessionId }
        }) as any[];

        for (const session of sessions) {
          await strapi.entityService.delete('plugin::llm-chat.chat-session', session.id);
        }

        // Supprimer de la cache
        conversationChains.delete(sessionId);

        return true;
      } catch (error) {
        strapi.log.error('Error clearing history:', error);
        return false;
      }
    },

    // Effacer toutes les conversations
    async clearAllHistory() {
      try {
        const sessions = await strapi.entityService.findMany('plugin::llm-chat.chat-session') as any[];
        const messages = await strapi.entityService.findMany('plugin::llm-chat.chat-message') as any[];

        // Supprimer tous les messages
        for (const message of messages) {
          await strapi.entityService.delete('plugin::llm-chat.chat-message', message.id);
        }

        // Supprimer toutes les sessions
        for (const session of sessions) {
          await strapi.entityService.delete('plugin::llm-chat.chat-session', session.id);
        }

        conversationChains.clear();

        return { cleared: sessions.length };
      } catch (error) {
        strapi.log.error('Error clearing all history:', error);
        return { cleared: 0 };
      }
    },

    // Nouvelle m√©thode pour le streaming
    async streamChat(message: string, options?: ConversationOptions) {
      try {
        const pluginConfig = strapi.config.get('plugin::llm-chat') || strapi.plugin('llm-chat').config('default');

        if (!pluginConfig) {
          throw new Error('LLM Chat plugin configuration not found');
        }

        const config = pluginConfig as LlmChatConfig;
        const sessionId = options?.sessionId || 'default';

        // Cr√©er le mod√®le avec streaming activ√©
        const temperature = options?.temperature !== undefined
          ? Number(options.temperature)
          : Number(config.provider === 'openai' ? config.openai.temperature : config.custom.temperature);

        let model;
        if (config.provider === 'openai') {
          model = new ChatOpenAI({
            modelName: config.openai.modelName,
            temperature: temperature,
            openAIApiKey: config.openai.apiKey,
            maxTokens: options?.maxTokens ? Number(options.maxTokens) : undefined,
            streaming: true,
          });
        } else if (config.provider === 'custom') {
          model = new ChatOpenAI({
            modelName: config.custom.modelName,
            temperature: temperature,
            maxTokens: options?.maxTokens ? Number(options.maxTokens) : undefined,
            streaming: true,
            configuration: {
              baseURL: config.custom.baseUrl,
              apiKey: config.custom.apiKey || "not-needed",
            },
          });
        } else {
          throw new Error(`Unsupported LLM provider: ${config.provider}`);
        }

        // R√©cup√©rer ou cr√©er une conversation
        if (!conversationChains.has(sessionId)) {
          const memory = new StrapiChatMemory(strapi, sessionId);

          const systemPrompt = options?.systemPrompt || "You are a helpful AI assistant.";
          const chatPrompt = ChatPromptTemplate.fromMessages([
            ["system", systemPrompt],
            new MessagesPlaceholder("history"),
            HumanMessagePromptTemplate.fromTemplate("{input}"),
          ]);

          const chain = new ConversationChain({
            memory: memory,
            prompt: chatPrompt,
            llm: model,
          });

          conversationChains.set(sessionId, chain);
        }

        const chain = conversationChains.get(sessionId);

        // Retourner un g√©n√©rateur pour le streaming
        return {
          async *stream() {
            let fullResponse = '';

            try {
              const stream = await chain.stream(
                { input: message },
                {
                  callbacks: [{
                    handleLLMNewToken(token) {
                      fullResponse += token;
                      return token;
                    }
                  }]
                }
              );

              for await (const chunk of stream) {
                if (chunk.response) {
                  yield `data: ${JSON.stringify({ content: chunk.response })}\n\n`;
                }
              }

              yield `data: [DONE]\n\n`;
            } catch (error) {
              yield `data: ${JSON.stringify({ error: error.message })}\n\n`;
            }
          },
          sessionId,
        };
      } catch (error) {
        strapi.log.error('Error in langchain stream service:', error);
        throw error;
      }
    },

    // Supprimer une session
    async deleteSession(sessionId: string) {
      try {
        const messages = await strapi.entityService.findMany('plugin::llm-chat.chat-message', {
          filters: { sessionId }
        }) as any[];

        for (const message of messages) {
          await strapi.entityService.delete('plugin::llm-chat.chat-message', message.id);
        }

        const sessions = await strapi.entityService.findMany('plugin::llm-chat.chat-session', {
          filters: { sessionId }
        }) as any[];

        for (const session of sessions) {
          await strapi.entityService.delete('plugin::llm-chat.chat-session', session.id);
        }

        conversationChains.delete(sessionId);

        return true;
      } catch (error) {
        strapi.log.error('Error deleting session:', error);
        return false;
      }
    },

    // Mettre √† jour le titre d'une session
    async updateSessionTitle(sessionId: string, title: string) {
      try {
        // V√©rifier si la session existe
        const existingSession = await strapi.entityService.findMany('plugin::llm-chat.chat-session', {
          filters: { sessionId }
        }) as any[];

        if (existingSession.length > 0) {
          // Mettre √† jour la session existante
          await strapi.entityService.update('plugin::llm-chat.chat-session', existingSession[0].id, {
            data: { title } as any
          });
        } else {
          // Cr√©er une nouvelle session
          await strapi.entityService.create('plugin::llm-chat.chat-session', {
            data: {
              sessionId,
              title,
              messageCount: 0,
              lastActivity: new Date().toISOString(),
              lastMessage: 'New session'
            } as any
          });
        }

        return true;
      } catch (error) {
        strapi.log.error('Error updating session title:', error);
        return false;
      }
    },
  };
};

export default langchainService;
