import type { Core } from '@strapi/strapi';

/**
 * Service pour int√©grer Ollama et ses mod√®les
 */
const ollamaService = ({ strapi }: { strapi: Core.Strapi }) => {

  // Configuration par d√©faut d'Ollama
  const getOllamaConfig = () => {
    const pluginConfig = strapi.config.get('plugin::llm-chat') || strapi.plugin('llm-chat').config('default');
    const config = pluginConfig as any; // Type assertion pour √©viter les erreurs TypeScript
    return {
      baseUrl: config?.custom?.baseUrl || 'http://localhost:11434',
      qwenModel: 'qwen3:0.6b'
    };
  };

  /**
   * Appelle le mod√®le qwen3:0.6b pour analyser si le RAG est n√©cessaire
   */
  const shouldUseRAGWithOllama = async (userMessage: string): Promise<{
    shouldUseRAG: boolean;
    confidence: number;
    keywords: string[];
    reasoning: string;
  }> => {
    const timerId = `üß† Ollama RAG Analysis [${Date.now()}]`;
    console.time(timerId);

    try {
      const config = getOllamaConfig();

      // Test rapide de connexion avant d'essayer l'analyse
      console.log('üîç Testing Ollama connection...');
      const isConnected = await testConnectionQuick();
      if (!isConnected) {
        console.log('‚ö†Ô∏è Ollama not available, using fallback analysis');
        const shouldUseRAG = manualKeywordAnalysis(userMessage);
        console.timeEnd(timerId);
        return {
          shouldUseRAG,
          confidence: 0.8,
          keywords: shouldUseRAG ? extractBasicKeywords(userMessage) : [],
          reasoning: 'Fallback analysis'
        };
      }

      // Prompt ultra-court optimis√© pour qwen3:0.6b
      const analysisPrompt = `Question: "${userMessage}"

Portfolio database contains: projects, skills, experience, education, contact.

JSON response format:
{"shouldUseRAG": true/false, "confidence": 0.9, "keywords": ["word1", "word2"]}

Examples:
"React projects?" ‚Üí {"shouldUseRAG": true, "confidence": 0.9, "keywords": ["projects", "React"]}
"Weather?" ‚Üí {"shouldUseRAG": false, "confidence": 0.9, "keywords": []}
"Contact?" ‚Üí {"shouldUseRAG": true, "confidence": 0.9, "keywords": ["contact"]}

Response:`;

      console.log('üß† Analyzing with Ollama qwen3:0.6b...');

      const requestBody = {
        model: config.qwenModel,
        prompt: analysisPrompt,
        stream: false,
        think: false,
        options: {
          temperature: 0.0, // Temp√©rature tr√®s basse pour des r√©ponses d√©terministes
          num_ctx: 512, // Contexte r√©duit de moiti√©
          top_p: 0.1, // Plus restrictif pour acc√©l√©rer
          top_k: 10, // Limite les choix pour acc√©l√©rer
          repeat_penalty: 1.0
        }
      };

      // Utiliser fetch sans AbortController comme dans langchain-service
      const response = await fetch(`${config.baseUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody)
      });

      if (!response.ok) {
        throw new Error(`Ollama request failed: ${response.status} ${response.statusText}`);
      }

      const data = await response.json() as any; // Type assertion pour √©viter les erreurs TypeScript
      const ollamaResponse = data.response;

      console.log('üéØ Ollama raw response:', ollamaResponse);

      // Parser la r√©ponse JSON
      const jsonMatch = ollamaResponse.match(/\{[\s\S]*\}/);
      if (!jsonMatch) {
        throw new Error('No JSON found in Ollama response');
      }

      const analysis = JSON.parse(jsonMatch[0]);

      // Validation et fallback
      const result = {
        shouldUseRAG: Boolean(analysis.shouldUseRAG),
        confidence: Math.max(0, Math.min(1, Number(analysis.confidence) || 0.9)),
        keywords: Array.isArray(analysis.keywords) ? analysis.keywords : [],
        reasoning: 'Fast analysis' // Reasoning simplifi√©
      };

      console.log('‚úÖ Ollama analysis result:', result);
      console.timeEnd(timerId);
      return result;

    } catch (error) {
      console.timeEnd(timerId);
      console.error('‚ùå Ollama analysis failed:', error);

      // Fallback √† l'analyse manuelle en cas d'erreur
      const shouldUseRAG = manualKeywordAnalysis(userMessage);

      return {
        shouldUseRAG,
        confidence: 0.8, // Confiance √©lev√©e pour le fallback
        keywords: shouldUseRAG ? extractBasicKeywords(userMessage) : [],
        reasoning: `Fallback: ${error instanceof Error ? error.message.substring(0, 50) : 'error'}`
      };
    }
  };

  /**
   * Analyse manuelle de fallback
   */
  const manualKeywordAnalysis = (message: string): boolean => {
    const portfolioKeywords = [
      'projet', 'projects', 'comp√©tence', 'skills', 'exp√©rience', 'experience',
      'formation', 'education', 'contact', 'portfolio', 'cv', 'profil',
      'react', 'vue', 'angular', 'php', 'python', 'javascript', 'typescript',
      'd√©velopp√©', 'd√©veloppement', 'cr√©√©', 'r√©alisation', 'technologies'
    ];

    const lowerMessage = message.toLowerCase();
    return portfolioKeywords.some(keyword => lowerMessage.includes(keyword));
  };

  /**
   * Extraction basique de mots-cl√©s pour le fallback
   */
  const extractBasicKeywords = (message: string): string[] => {
    const techKeywords = ['react', 'vue', 'angular', 'php', 'python', 'javascript', 'typescript'];
    const contextKeywords = ['projet', 'comp√©tence', 'exp√©rience', 'formation', 'contact'];

    const lowerMessage = message.toLowerCase();
    const found: string[] = [];

    [...techKeywords, ...contextKeywords].forEach(keyword => {
      if (lowerMessage.includes(keyword)) {
        found.push(keyword);
      }
    });

    return found;
  };

  /**
   * Test de connexion Ollama rapide
   */
  const testConnectionQuick = async (): Promise<boolean> => {
    try {
      const config = getOllamaConfig();
      const response = await fetch(`${config.baseUrl}/api/tags`, {
        method: 'GET'
      });

      return response.ok;
    } catch (error) {
      return false;
    }
  };

  /**
   * Test de connexion Ollama
   */
  const testConnection = async (): Promise<boolean> => {
    try {
      const config = getOllamaConfig();
      const response = await fetch(`${config.baseUrl}/api/tags`, {
        method: 'GET'
      });

      return response.ok;
    } catch (error) {
      console.error('‚ùå Ollama connection test failed:', error);
      return false;
    }
  };

  /**
   * V√©rifier si le mod√®le qwen3:0.6b est disponible
   */
  const checkQwenModel = async (): Promise<boolean> => {
    try {
      const config = getOllamaConfig();
      const response = await fetch(`${config.baseUrl}/api/tags`);

      if (!response.ok) return false;

      const data = await response.json() as any; // Type assertion pour √©viter les erreurs TypeScript
      const models = data.models || [];

      return models.some((model: any) => model.name === config.qwenModel);
    } catch (error) {
      console.error('‚ùå Failed to check qwen model:', error);
      return false;
    }
  };

  return {
    shouldUseRAGWithOllama,
    testConnection,
    testConnectionQuick,
    checkQwenModel,
    getOllamaConfig
  };
};

export default ollamaService;
